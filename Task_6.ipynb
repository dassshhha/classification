{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLy2I8x-bQEk"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
        "- `news_train.txt` тестовое множество\n",
        "- `news_test.txt` тренировочное множество\n",
        "\n",
        "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
        "\n",
        "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
        "\n",
        ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX7EjZ-bbQE0"
      },
      "source": [
        "# Задача\n",
        "\n",
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "    \n",
        "    \n",
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
        "\n",
        "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия\n",
        "    \n",
        "\n",
        "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TASK 1"
      ],
      "metadata": {
        "id": "sKyz_XBOUjSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gsu7EnvVhjeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iM6zADOzhnlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"/content/drive/MyDrive/news/news_train.txt\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/news/news_test.txt\""
      ],
      "metadata": {
        "id": "vpLmBB1lw49S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(TRAIN_PATH, sep=\"\\t\", header=None, names=['topic', 'headline', 'info'])\n",
        "test_data = pd.read_csv(TEST_PATH, sep=\"\\t\", header=None, names=['topic', 'headline', 'info'])"
      ],
      "metadata": {
        "id": "9nm545MT2E5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "A3ie6YdN2Sdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "\n",
        "import pymorphy2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "rus_stopwords = stopwords.words(\"russian\")\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "BTRAydhSJgs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text):\n",
        "  punctuation = string.punctuation + '«' + '»' + '—'\n",
        "  text = \"\".join(ch if ch not in punctuation else ' ' for ch in text)\n",
        "  text = \"\".join([ch if not ch.isdigit() else ' ' for ch in text])\n",
        "  words = text.split()\n",
        "  tokens = []\n",
        "  for word in words:\n",
        "    word = morph.parse(word)[0].normal_form\n",
        "    if word not in rus_stopwords:\n",
        "      tokens.append(word)\n",
        "  text = \" \".join(tokens)\n",
        "  return text"
      ],
      "metadata": {
        "id": "cxfoeP1RKMZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_data)):\n",
        "  train_data[\"headline\"][i] = text_preprocessing(train_data[\"headline\"][i])\n",
        "  train_data[\"info\"][i] = text_preprocessing(train_data[\"info\"][i])"
      ],
      "metadata": {
        "id": "n5XUQxq_TSQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "ofJX1YqnTxJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_data)):\n",
        "  test_data[\"headline\"][i] = text_preprocessing(test_data[\"headline\"][i])\n",
        "  test_data[\"info\"][i] = text_preprocessing(test_data[\"info\"][i])"
      ],
      "metadata": {
        "id": "wZ_pvt00UBi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = pd.read_csv(\"/content/drive/MyDrive/news/preprocessed_train.txt\")\n",
        "# test_data = pd.read_csv(\"/content/drive/MyDrive/news/preprocessed_test.txt\")"
      ],
      "metadata": {
        "id": "g-DKKy2wJfOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "train_data['target'] = label_encoder.fit_transform(train_data['topic'])\n",
        "test_data['target'] = label_encoder.transform(test_data['topic'])"
      ],
      "metadata": {
        "id": "qgMaPLADaNTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data.to_csv( \"/content/drive/MyDrive/news/preprocessed_train.txt\", index=False, encoding='utf-8-sig')\n",
        "# test_data.to_csv( \"/content/drive/MyDrive/news/preprocessed_test.txt\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "udsnQrvFZ4fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WFv09c23rp42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TASK 2"
      ],
      "metadata": {
        "id": "tA948ugYYNbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "D7AfVigcYPeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [text.split() for text in train_data[\"info\"]]\n",
        "w2v = Word2Vec(sentences=words, min_count=0)"
      ],
      "metadata": {
        "id": "4Mhyz4fgYYMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar('интернет', topn=10)  "
      ],
      "metadata": {
        "id": "vDFn7Y5UZFF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TASK 3"
      ],
      "metadata": {
        "id": "N6aAdBfBacfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "xYF2t5PBxkG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(min_df = 10, max_df = 1., ngram_range = (1,2), max_features = 1000)\n",
        "train_text = vectorizer.fit_transform(train_data[\"info\"]).toarray()\n",
        "test_text = vectorizer.transform(test_data[\"info\"]).toarray()\n",
        "train_labels = np.array(train_data['target'])\n",
        "test_labels = np.array(test_data['target'])"
      ],
      "metadata": {
        "id": "xV68vg_R9Ipn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1],'kernel': ['linear', 'sigmoid']}\n",
        "model = SVC()\n",
        "clf = GridSearchCV(model, param_grid, cv=2, verbose=3)\n",
        "clf.fit(train_text, train_labels)"
      ],
      "metadata": {
        "id": "YNUBB6WVjKAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(test_text)\n",
        "print('SVC accuracy: ', accuracy_score(pred, test_labels))"
      ],
      "metadata": {
        "id": "7js9EkrbBrGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'solver':['liblinear','saga'], 'penalty': ['l1', 'l2']}\n",
        "model = LogisticRegression()\n",
        "clf = GridSearchCV(model, param_grid, cv=2, verbose=3)\n",
        "clf.fit(train_text, train_labels)"
      ],
      "metadata": {
        "id": "4QM6zz5MA5TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(test_text)\n",
        "print('Logistic Regression accuracy: ', accuracy_score(pred, test_labels))"
      ],
      "metadata": {
        "id": "UxzX7a7rB-PM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Task_6.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}